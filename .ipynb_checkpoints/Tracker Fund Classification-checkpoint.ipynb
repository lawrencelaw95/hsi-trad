{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction As Classification\n",
    "Continuing the 2800-HK price prediction from classification perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules and load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import sklearn\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Keras module\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load stored variables upon start\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretty plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>P_L1</th>\n",
       "      <th>P_L2</th>\n",
       "      <th>P_L3</th>\n",
       "      <th>P_L4</th>\n",
       "      <th>P_L5</th>\n",
       "      <th>P_L6</th>\n",
       "      <th>P_L7</th>\n",
       "      <th>P_L8</th>\n",
       "      <th>P_L9</th>\n",
       "      <th>...</th>\n",
       "      <th>P_L11</th>\n",
       "      <th>P_L12</th>\n",
       "      <th>P_L13</th>\n",
       "      <th>P_L14</th>\n",
       "      <th>P_L15</th>\n",
       "      <th>P_L16</th>\n",
       "      <th>P_L17</th>\n",
       "      <th>P_L18</th>\n",
       "      <th>P_L19</th>\n",
       "      <th>P_L20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>...</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>...</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>...</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>...</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                P   P_L1   P_L2   P_L3   P_L4   P_L5   P_L6   P_L7   P_L8  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70  22.05   \n",
       "2007-09-03  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70   \n",
       "2007-09-04  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35   \n",
       "2007-09-05  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30   \n",
       "2007-09-06  24.50  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90   \n",
       "\n",
       "             P_L9  ...    P_L11  P_L12  P_L13  P_L14  P_L15  P_L16  P_L17  \\\n",
       "Date               ...                                                      \n",
       "2007-08-31  21.85  ...    20.95  21.70  22.35  22.30  22.05  22.70  22.85   \n",
       "2007-09-03  22.05  ...    20.80  20.95  21.70  22.35  22.30  22.05  22.70   \n",
       "2007-09-04  22.70  ...    21.85  20.80  20.95  21.70  22.35  22.30  22.05   \n",
       "2007-09-05  23.35  ...    22.05  21.85  20.80  20.95  21.70  22.35  22.30   \n",
       "2007-09-06  23.30  ...    22.70  22.05  21.85  20.80  20.95  21.70  22.35   \n",
       "\n",
       "            P_L18  P_L19  P_L20  \n",
       "Date                             \n",
       "2007-08-31  22.15  22.25  22.85  \n",
       "2007-09-03  22.85  22.15  22.25  \n",
       "2007-09-04  22.70  22.85  22.15  \n",
       "2007-09-05  22.05  22.70  22.85  \n",
       "2007-09-06  22.30  22.05  22.70  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import price data\n",
    "#Load the historical prices of 2800-HK, with lags 1 to lag 20\n",
    "price_hist_data = pd.read_csv('price_only.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "price_hist_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Bid</th>\n",
       "      <th>20D Vol</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA15</th>\n",
       "      <th>MA12</th>\n",
       "      <th>MA20</th>\n",
       "      <th>...</th>\n",
       "      <th>DY_LTM</th>\n",
       "      <th>DY_NTM</th>\n",
       "      <th>ADV_VOL</th>\n",
       "      <th>PAYOUT</th>\n",
       "      <th>ANALYST_SENTIMENT</th>\n",
       "      <th>EPS_GRW_FY1</th>\n",
       "      <th>EPS_GRW_FY2</th>\n",
       "      <th>PE_NTM</th>\n",
       "      <th>PE_LTM</th>\n",
       "      <th>C2D_LTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.367823</td>\n",
       "      <td>23.82</td>\n",
       "      <td>22.841667</td>\n",
       "      <td>22.696667</td>\n",
       "      <td>22.6225</td>\n",
       "      <td>...</td>\n",
       "      <td>2.782797</td>\n",
       "      <td>2.880128</td>\n",
       "      <td>99.419898</td>\n",
       "      <td>48.051962</td>\n",
       "      <td>1.909071</td>\n",
       "      <td>30.724490</td>\n",
       "      <td>2.717280</td>\n",
       "      <td>16.695058</td>\n",
       "      <td>16.975805</td>\n",
       "      <td>58.050474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.266743</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.120832</td>\n",
       "      <td>22.830000</td>\n",
       "      <td>22.7250</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784563</td>\n",
       "      <td>2.892308</td>\n",
       "      <td>11.570023</td>\n",
       "      <td>48.305678</td>\n",
       "      <td>1.835359</td>\n",
       "      <td>31.005439</td>\n",
       "      <td>1.876752</td>\n",
       "      <td>16.709085</td>\n",
       "      <td>16.925571</td>\n",
       "      <td>58.154835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.259649</td>\n",
       "      <td>24.02</td>\n",
       "      <td>23.412500</td>\n",
       "      <td>22.960000</td>\n",
       "      <td>22.8325</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784339</td>\n",
       "      <td>2.894377</td>\n",
       "      <td>69.555725</td>\n",
       "      <td>48.307102</td>\n",
       "      <td>1.725886</td>\n",
       "      <td>31.040968</td>\n",
       "      <td>1.891823</td>\n",
       "      <td>16.697662</td>\n",
       "      <td>16.919413</td>\n",
       "      <td>58.177640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.172140</td>\n",
       "      <td>24.22</td>\n",
       "      <td>23.620832</td>\n",
       "      <td>23.136667</td>\n",
       "      <td>22.9075</td>\n",
       "      <td>...</td>\n",
       "      <td>2.767348</td>\n",
       "      <td>2.867314</td>\n",
       "      <td>24.265623</td>\n",
       "      <td>48.267351</td>\n",
       "      <td>1.741908</td>\n",
       "      <td>30.716929</td>\n",
       "      <td>1.878484</td>\n",
       "      <td>16.841225</td>\n",
       "      <td>17.017692</td>\n",
       "      <td>58.195446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>2.160638</td>\n",
       "      <td>24.36</td>\n",
       "      <td>23.825000</td>\n",
       "      <td>23.373333</td>\n",
       "      <td>22.9975</td>\n",
       "      <td>...</td>\n",
       "      <td>2.775339</td>\n",
       "      <td>2.878880</td>\n",
       "      <td>88.845002</td>\n",
       "      <td>48.288397</td>\n",
       "      <td>1.696151</td>\n",
       "      <td>31.336520</td>\n",
       "      <td>1.861943</td>\n",
       "      <td>16.780660</td>\n",
       "      <td>16.975485</td>\n",
       "      <td>58.224743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close   High    Low    Ask    Bid   20D Vol    MA5       MA15  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  24.35  24.35  24.35  24.35  2.367823  23.82  22.841667   \n",
       "2007-09-03  24.30  24.30  24.30  24.30  24.30  2.266743  23.90  23.120832   \n",
       "2007-09-04  24.30  24.30  24.30  24.30  24.30  2.259649  24.02  23.412500   \n",
       "2007-09-05  24.35  24.35  24.35  24.35  24.35  2.172140  24.22  23.620832   \n",
       "2007-09-06  24.50  24.50  24.50  24.50  24.50  2.160638  24.36  23.825000   \n",
       "\n",
       "                 MA12     MA20    ...        DY_LTM    DY_NTM    ADV_VOL  \\\n",
       "Date                              ...                                      \n",
       "2007-08-31  22.696667  22.6225    ...      2.782797  2.880128  99.419898   \n",
       "2007-09-03  22.830000  22.7250    ...      2.784563  2.892308  11.570023   \n",
       "2007-09-04  22.960000  22.8325    ...      2.784339  2.894377  69.555725   \n",
       "2007-09-05  23.136667  22.9075    ...      2.767348  2.867314  24.265623   \n",
       "2007-09-06  23.373333  22.9975    ...      2.775339  2.878880  88.845002   \n",
       "\n",
       "               PAYOUT  ANALYST_SENTIMENT  EPS_GRW_FY1  EPS_GRW_FY2     PE_NTM  \\\n",
       "Date                                                                            \n",
       "2007-08-31  48.051962           1.909071    30.724490     2.717280  16.695058   \n",
       "2007-09-03  48.305678           1.835359    31.005439     1.876752  16.709085   \n",
       "2007-09-04  48.307102           1.725886    31.040968     1.891823  16.697662   \n",
       "2007-09-05  48.267351           1.741908    30.716929     1.878484  16.841225   \n",
       "2007-09-06  48.288397           1.696151    31.336520     1.861943  16.780660   \n",
       "\n",
       "               PE_LTM    C2D_LTM  \n",
       "Date                              \n",
       "2007-08-31  16.975805  58.050474  \n",
       "2007-09-03  16.925571  58.154835  \n",
       "2007-09-04  16.919413  58.177640  \n",
       "2007-09-05  17.017692  58.195446  \n",
       "2007-09-06  16.975485  58.224743  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Fundamentals Data\n",
    "fund_data = pd.read_csv('new_index_data.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "fund_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hang Seng Index</th>\n",
       "      <th>SSE Composite Index</th>\n",
       "      <th>ASX All Ordinaries</th>\n",
       "      <th>India S&amp;P BSE SENSEX</th>\n",
       "      <th>TOPIX</th>\n",
       "      <th>KOSPI Composite Index</th>\n",
       "      <th>Taiwan TAIEX</th>\n",
       "      <th>FTSE Bursa Malaysia KLCI</th>\n",
       "      <th>FTSE Straits Times Index</th>\n",
       "      <th>Philippines PSE PSEi</th>\n",
       "      <th>...</th>\n",
       "      <th>Turkey BIST 100</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>DJ Industrial Average</th>\n",
       "      <th>Colombia IGBC</th>\n",
       "      <th>Canada S&amp;P/TSX Composite</th>\n",
       "      <th>Brazil Bovespa Index</th>\n",
       "      <th>Mexico IPC</th>\n",
       "      <th>Israel TA-125</th>\n",
       "      <th>Saudi Arabia All Share (TASI)</th>\n",
       "      <th>FTSE JSE All Share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>23984.14</td>\n",
       "      <td>5218.825</td>\n",
       "      <td>6248.3</td>\n",
       "      <td>15318.60</td>\n",
       "      <td>1608.25</td>\n",
       "      <td>1873.24</td>\n",
       "      <td>8982.16</td>\n",
       "      <td>1273.93</td>\n",
       "      <td>3328.43</td>\n",
       "      <td>3365.29</td>\n",
       "      <td>...</td>\n",
       "      <td>50198.60</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10728.74</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54637.24</td>\n",
       "      <td>30347.86</td>\n",
       "      <td>1034.67</td>\n",
       "      <td>8226.97</td>\n",
       "      <td>28660.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>23904.09</td>\n",
       "      <td>5321.055</td>\n",
       "      <td>6272.5</td>\n",
       "      <td>15422.05</td>\n",
       "      <td>1605.44</td>\n",
       "      <td>1881.81</td>\n",
       "      <td>8979.96</td>\n",
       "      <td>1284.14</td>\n",
       "      <td>3321.36</td>\n",
       "      <td>3369.14</td>\n",
       "      <td>...</td>\n",
       "      <td>49936.94</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10750.79</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54832.51</td>\n",
       "      <td>30797.60</td>\n",
       "      <td>1047.33</td>\n",
       "      <td>8017.54</td>\n",
       "      <td>28887.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>23886.07</td>\n",
       "      <td>5294.045</td>\n",
       "      <td>6297.1</td>\n",
       "      <td>15465.40</td>\n",
       "      <td>1596.74</td>\n",
       "      <td>1874.74</td>\n",
       "      <td>8922.98</td>\n",
       "      <td>1283.75</td>\n",
       "      <td>3308.81</td>\n",
       "      <td>3312.30</td>\n",
       "      <td>...</td>\n",
       "      <td>50032.59</td>\n",
       "      <td>1489.42</td>\n",
       "      <td>13448.86</td>\n",
       "      <td>10880.85</td>\n",
       "      <td>13755.23</td>\n",
       "      <td>55250.47</td>\n",
       "      <td>30932.71</td>\n",
       "      <td>1054.69</td>\n",
       "      <td>7878.70</td>\n",
       "      <td>29051.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24069.17</td>\n",
       "      <td>5310.716</td>\n",
       "      <td>6274.3</td>\n",
       "      <td>15446.15</td>\n",
       "      <td>1569.47</td>\n",
       "      <td>1865.59</td>\n",
       "      <td>8913.85</td>\n",
       "      <td>1297.93</td>\n",
       "      <td>3375.02</td>\n",
       "      <td>3342.35</td>\n",
       "      <td>...</td>\n",
       "      <td>49421.38</td>\n",
       "      <td>1472.29</td>\n",
       "      <td>13305.47</td>\n",
       "      <td>10819.91</td>\n",
       "      <td>13683.28</td>\n",
       "      <td>54407.83</td>\n",
       "      <td>30809.55</td>\n",
       "      <td>1048.70</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28696.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24050.40</td>\n",
       "      <td>5393.660</td>\n",
       "      <td>6265.3</td>\n",
       "      <td>15616.31</td>\n",
       "      <td>1568.52</td>\n",
       "      <td>1888.81</td>\n",
       "      <td>9017.08</td>\n",
       "      <td>1298.85</td>\n",
       "      <td>3399.49</td>\n",
       "      <td>3326.53</td>\n",
       "      <td>...</td>\n",
       "      <td>49601.39</td>\n",
       "      <td>1478.55</td>\n",
       "      <td>13363.35</td>\n",
       "      <td>10844.40</td>\n",
       "      <td>13795.69</td>\n",
       "      <td>54569.00</td>\n",
       "      <td>30816.95</td>\n",
       "      <td>1033.23</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28850.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hang Seng Index  SSE Composite Index  ASX All Ordinaries  \\\n",
       "Date                                                                   \n",
       "2007-08-31         23984.14             5218.825              6248.3   \n",
       "2007-09-03         23904.09             5321.055              6272.5   \n",
       "2007-09-04         23886.07             5294.045              6297.1   \n",
       "2007-09-05         24069.17             5310.716              6274.3   \n",
       "2007-09-06         24050.40             5393.660              6265.3   \n",
       "\n",
       "            India S&P BSE SENSEX    TOPIX  KOSPI Composite Index  \\\n",
       "Date                                                               \n",
       "2007-08-31              15318.60  1608.25                1873.24   \n",
       "2007-09-03              15422.05  1605.44                1881.81   \n",
       "2007-09-04              15465.40  1596.74                1874.74   \n",
       "2007-09-05              15446.15  1569.47                1865.59   \n",
       "2007-09-06              15616.31  1568.52                1888.81   \n",
       "\n",
       "            Taiwan TAIEX  FTSE Bursa Malaysia KLCI  FTSE Straits Times Index  \\\n",
       "Date                                                                           \n",
       "2007-08-31       8982.16                   1273.93                   3328.43   \n",
       "2007-09-03       8979.96                   1284.14                   3321.36   \n",
       "2007-09-04       8922.98                   1283.75                   3308.81   \n",
       "2007-09-05       8913.85                   1297.93                   3375.02   \n",
       "2007-09-06       9017.08                   1298.85                   3399.49   \n",
       "\n",
       "            Philippines PSE PSEi         ...          Turkey BIST 100  \\\n",
       "Date                                     ...                            \n",
       "2007-08-31               3365.29         ...                 50198.60   \n",
       "2007-09-03               3369.14         ...                 49936.94   \n",
       "2007-09-04               3312.30         ...                 50032.59   \n",
       "2007-09-05               3342.35         ...                 49421.38   \n",
       "2007-09-06               3326.53         ...                 49601.39   \n",
       "\n",
       "            S&P 500  DJ Industrial Average  Colombia IGBC  \\\n",
       "Date                                                        \n",
       "2007-08-31  1473.99               13357.74       10728.74   \n",
       "2007-09-03  1473.99               13357.74       10750.79   \n",
       "2007-09-04  1489.42               13448.86       10880.85   \n",
       "2007-09-05  1472.29               13305.47       10819.91   \n",
       "2007-09-06  1478.55               13363.35       10844.40   \n",
       "\n",
       "            Canada S&P/TSX Composite  Brazil Bovespa Index  Mexico IPC  \\\n",
       "Date                                                                     \n",
       "2007-08-31                  13660.48              54637.24    30347.86   \n",
       "2007-09-03                  13660.48              54832.51    30797.60   \n",
       "2007-09-04                  13755.23              55250.47    30932.71   \n",
       "2007-09-05                  13683.28              54407.83    30809.55   \n",
       "2007-09-06                  13795.69              54569.00    30816.95   \n",
       "\n",
       "            Israel TA-125  Saudi Arabia All Share (TASI)  FTSE JSE All Share  \n",
       "Date                                                                          \n",
       "2007-08-31        1034.67                        8226.97            28660.35  \n",
       "2007-09-03        1047.33                        8017.54            28887.48  \n",
       "2007-09-04        1054.69                        7878.70            29051.96  \n",
       "2007-09-05        1048.70                        7853.66            28696.67  \n",
       "2007-09-06        1033.23                        7853.66            28850.19  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Global index data\n",
    "idx_data = pd.read_csv('indices.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "idx_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-process Raw Data\n",
    "1. Generate labels\n",
    "2. Apply lags to global index data\n",
    "3. Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Labels from Price History Data\n",
    "#Generate UP/DOWN labels from log change\n",
    "cutoff_perc = 0.0005 #0.05% return as cuttoff to define UP label\n",
    "lag = 1 #forward returns\n",
    "\n",
    "labels = np.zeros([price_hist_data.shape[0]])\n",
    "\n",
    "#Caluclate log-returns\n",
    "ret = np.log(price_hist_data['P'].shift(-lag)/price_hist_data['P'])\n",
    "labels = [1 if r > cutoff_perc else 0 for r in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying lags to index data\n",
    "#Seperate the indices into 2 classes - lag or no_lag\n",
    "no_lag = [0, 1, 2, 4, 5, 6, 9, 10]\n",
    "lag = [i for i in range(0,idx_data.shape[1]) if i not in no_lag]\n",
    "\n",
    "#Processing the dataset by applying appropriate lags\n",
    "lagged_data = idx_data.iloc[:,lag].shift(1)\n",
    "idx_data = pd.concat([idx_data.iloc[:,no_lag], lagged_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove first row\n",
    "idx_data = idx_data.iloc[1:, :]\n",
    "price_hist_data = price_hist_data.iloc[1:, :]\n",
    "fund_data = fund_data.iloc[1:, :]\n",
    "labels=labels[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of idx_data:  (2483, 42)\n",
      "Shape of price_hist_data:  (2483, 21)\n",
      "Shape of fund_data:  (2483, 30)\n",
      "Shape of labels:  2483\n"
     ]
    }
   ],
   "source": [
    "#Check Dimensions to make sure everythings right before continuing..\n",
    "print(\"Shape of idx_data: \", idx_data.shape)\n",
    "print(\"Shape of price_hist_data: \", price_hist_data.shape)\n",
    "print(\"Shape of fund_data: \", fund_data.shape)\n",
    "print(\"Shape of labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "X = np.array(pd.concat([price_hist_data, idx_data, fund_data], axis=1))\n",
    "y = to_categorical(labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking whether there are NAs\n",
    "[np.sum(np.isnan(X), axis=0) > 0] == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split training, validation and test set\n",
    "\n",
    "In this stage we have a look-ahead bias free set of data (X) and the labels y. Next, we will need to:\n",
    "- Normalize the input data. To avoid look ahead bias, we will z-score the features, using ONLY the training set.\n",
    "- Next, generate input data into LSTM network. We will need an overlapping sequence at 1-day window as input samples. Specifically suppose the *timestep* is 240, we will have a list of array consists of *number of rows of X* - 240 entries, each element has dimentions (240, num_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to split raw data into training, validation and test set, returns a numpy array.\n",
    "def split_data(input_data=[], train_size=0.8, val_size=0.2, test_size=0):\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #PARAM: input_data: numpy nd array\n",
    "    #PARAM: training_size: size of training set in decimal\n",
    "    #PARAM: val_size: size of validation set in decimal\n",
    "    #PARAM: test_size: size of test set in decimal\n",
    "    #OUTPUT: tuple (train_set, validation_set, test_set)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    #First check whether traing_size + val_size + test_size = 1 and each of the entries are positive\n",
    "    assert(train_size + val_size + test_size==1), \"Sum of training, validation and test size needs to be 1!\"\n",
    "    assert(train_size * val_size * test_size > 0), \"Sizes have to be positive!\"\n",
    "    \n",
    "    #Check input_data type is numpy array, after casting\n",
    "    if type(input_data) != 'numpy.ndarray':\n",
    "        input_data = np.array(input_data) \n",
    "    \n",
    "    assert(isinstance(input_data, np.ndarray)), \"Input has to be a numpy array!\"\n",
    "    \n",
    "    \n",
    "    #Calculate cut-off points\n",
    "    train_cut_index = int(train_size * input_data.shape[0])\n",
    "    val_cut_index = train_cut_index + int(val_size * input_data.shape[0])\n",
    "    \n",
    "    #Split the data\n",
    "    if len(input_data.shape) == 1:\n",
    "        train, val, test = input_data[:train_cut_index], input_data[train_cut_index:val_cut_index], input_data[val_cut_index:]\n",
    "    else:\n",
    "        train, val, test = input_data[:train_cut_index,:], input_data[train_cut_index:val_cut_index, :], input_data[val_cut_index:, :]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1489, 93)\n",
      "(496, 93)\n",
      "(498, 93)\n",
      "(1489, 2)\n",
      "(496, 2)\n",
      "(498, 2)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#Split data\n",
    "X_train, X_val, X_test = split_data(X, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "y_train, y_val, y_test = split_data(y, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to re-structure the data to get batches. Re-shape the data to have overlapping training set for time-series learning.\n",
    "def get_inputs(input_data, labels, batch_size, timesteps):\n",
    "    \n",
    "    #First get the total number of samples generated\n",
    "    n_seq = input_data.shape[0] - timesteps + 1\n",
    "    \n",
    "    #features, classes\n",
    "    n_dim = input_data.shape[1]\n",
    "    n_class = labels.shape[1]\n",
    "    \n",
    "    #Calculate the number of batches possible\n",
    "    n_samples = n_seq * timesteps\n",
    "    n_batches = n_samples // (batch_size * timesteps)\n",
    "\n",
    "    output = []\n",
    "    targets = []\n",
    "        \n",
    "    for jj in range(0, n_batches):\n",
    "    #Generate the sequences\n",
    "        \n",
    "        #if jj == n_batches:\n",
    "            \n",
    "            #output.append([input_data[jj*batch_size:, :]])\n",
    "            #targets.append([labels[jj*batch_size:, :]])            \n",
    "            #yield np.vstack(output), np.vstack(targets  )\n",
    "            \n",
    "        #else:            \n",
    "        for ii in range(jj * batch_size, (jj + 1) * batch_size):                    \n",
    "            #Getting the overlapping samples\n",
    "            output.append([scale(input_data[ii:ii + timesteps, :])])\n",
    "            targets.append([labels[ii:ii+timesteps, :]])\n",
    "\n",
    "    return np.vstack(output), np.vstack(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 5)\n",
      "(6, 4, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#get inputs\n",
    "t = np.reshape(np.arange(1,51), (10,5))\n",
    "s = np.reshape(np.arange(1,21), (10,2))\n",
    "\n",
    "#for (x, y) in get_inputs(t,s, 3, 4):\n",
    "#    print('training size: ', x.shape)\n",
    "#    print('training: \\n', x)\n",
    "#    print('label size: ', y.shape)\n",
    "#    print('labels: \\n', y)\n",
    "\n",
    "test_train, test_label = get_inputs(t,s,3,4)\n",
    "    \n",
    "\n",
    "#print(test_train[0:1])\n",
    "#print(test_label[0:1])\n",
    "#print(t)\n",
    "print(test_train.shape)\n",
    "print(test_label.shape)\n",
    "#print(test_train)\n",
    "#test_train.reshape((-1, 4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "learning_rate = 0.0005\n",
    "epochs= 1\n",
    "loss = 'binary_crossentropy'\n",
    "batch_size = 128\n",
    "timesteps = 22\n",
    "n_dim = X.shape[1]\n",
    "n_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer - using Adam Optimizer\n",
    "optimizer = optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Inputs\n",
    "X_train_1, y_train_1 = get_inputs(X_train, y_train, batch_size, timesteps)\n",
    "X_val_1, y_val_1 = get_inputs(X_val, y_val, batch_size, timesteps)\n",
    "X_test_1, y_test_1 = get_inputs(X_test, y_test, batch_size, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSTM Network object\n",
    "def build_network(n_hidden_layer, dropout, input_shape, batch_size, return_sequences=True, stateful=True):\n",
    "    \n",
    "    #Define network architect\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden_layer[0], input_shape=input_shape, batch_size=batch_size, return_sequences=return_sequences, stateful=stateful))\n",
    "    model.add(Dropout(dropout[0]))\n",
    "    model.add(LSTM(n_hidden_layer[1], return_sequences=return_sequences, stateful=stateful))\n",
    "    model.add(Dropout(dropout[0]))\n",
    "    model.add(Dense(n_hidden_layer[2], activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lstm1 = build_network([5,5,10], [0.1,0.1], input_shape=(timesteps, n_dim), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile\n",
    "#lstm1.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitted = lstm1.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Curve\n",
    "plt.figure()\n",
    "plt.plot(fitted.history['loss'])\n",
    "plt.plot(fitted.history['val_loss'])\n",
    "plt.title('Training Loss & Validation Loss')\n",
    "plt.ylabel('Binary Cross Entropy Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Accuracy\n",
    "plt.figure()\n",
    "plt.plot(fitted.history['acc'])\n",
    "plt.plot(fitted.history['val_acc'])\n",
    "plt.title('Training Accuracy & Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 6: Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search the following parameters:\n",
    "- Learning Rate & decay rate\n",
    "- Number of Hidden Layers\n",
    "- Dropout Rate\n",
    "- Timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grid Search for learning rate, at fixed 100 epochs.\n",
    "#lr = np.linspace(1e-5, 1e-3, 1)\n",
    "lr = np.linspace(1e-5, 1e-3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, learning rate: 0.000010, Best Validation Loss: 0.6985\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "lr_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Build network\n",
    "lr_network = build_network([5,5,10], [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for learning_rate in lr:\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    #Compile\n",
    "    lr_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_lr = lr_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d, learning rate: %.6f, Best Validation Loss: %.4f' %(steps, learning_rate, np.min(fitted_lr.history['val_loss'])))\n",
    "    \n",
    "    #Save\n",
    "    lr_loss.append([np.min(fitted_lr.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate: 0.00001\n",
      "Validation Loss: 0.00000\n",
      "Stored 'best_lr' (float64)\n",
      "Stored 'lr_loss' (list)\n"
     ]
    }
   ],
   "source": [
    "#Best Learning Rate\n",
    "best_lr = lr[np.argmin(lr_loss)]\n",
    "print('Best Learning Rate: %.5f' % best_lr)\n",
    "print('Validation Loss: %.5f' % np.argmin(lr_loss))\n",
    "%store best_lr\n",
    "%store lr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x175876a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot Learning Rate vs Validation Loss\n",
    "plt.plot(lr*1e4, lr_loss)\n",
    "plt.title('Learning Rate vs Validation Loss')\n",
    "plt.xlabel('Learning Rate (x 10^-4)')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate number of layers\n",
    "n_hidden_layers = np.arange(5,31)\n",
    "n_dense = np.arange(5,31)\n",
    "n_layers = list(itertools.product(n_hidden_layers, n_hidden_layers, n_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, Number of layers: (5, 5, 5),  Best Validation Loss: 0.6955\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "hd_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Build network\n",
    "\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for (hd1, hd2, d) in n_layers:\n",
    "    \n",
    "    hd_network = build_network([hd1, hd2, d], [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    hd_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_hidden = hd_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Number of layers: (%d, %d, %d), ' % (hd1, hd2, d), 'Best Validation Loss: %.4f' %  np.min(fitted_hidden.history['val_loss']))\n",
    "    \n",
    "    #Save\n",
    "    hd_loss.append([np.min(fitted_hidden.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Hidden Layer:  (5, 5, 5)  Validation Loss: 0.6955 \n",
      "Stored 'best_n_hidden' (tuple)\n",
      "Stored 'best_hidden_loss' (float64)\n"
     ]
    }
   ],
   "source": [
    "#Best number of hidden layer\n",
    "best_n_hidden = n_layers[np.argmin(hd_loss)]\n",
    "best_hidden_loss = np.min(hd_loss)\n",
    "print('Best Number of Hidden Layer: ', best_n_hidden, \" Validation Loss: %.4f \" % best_hidden_loss)\n",
    "%store best_n_hidden\n",
    "%store best_hidden_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Drop out rate\n",
    "drop_out = list(itertools.combinations_with_replacement(np.linspace(0.1,0.9, num=34),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, Dropout: (0.1000, 0.1000)  Best Validation Loss: 0.6967\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "do_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Build network\n",
    "\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for d in drop_out:\n",
    "    \n",
    "    do_network = build_network(best_n_hidden, d, input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    do_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_dropout = do_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' %steps, 'Dropout: (%.4f, %.4f) ' % d, 'Best Validation Loss: %.4f' % np.min(fitted_dropout.history['val_loss']))\n",
    "    \n",
    "    #Save\n",
    "    do_loss.append([np.min(fitted_dropout.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Drop out probability: (0.10, 0.10),  Validation Loss: 0.6967\n",
      "Stored 'best_drop_out' (tuple)\n",
      "Stored 'best_do_loss' (float64)\n"
     ]
    }
   ],
   "source": [
    "best_drop_out = drop_out[np.argmin(do_loss)]\n",
    "best_do_loss = np.min(do_loss)\n",
    "print('Best Drop out probability: (%.2f, %.2f), ' % best_drop_out, 'Validation Loss: %.4f' % best_do_loss)\n",
    "\n",
    "%store best_drop_out\n",
    "%store best_do_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test timesteps from 1 step to 220 days lag\n",
    "time = np.arange(10,220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, Timesteps: 10 Best Validation Loss: 0.6870\n",
      "Steps: 2, Timesteps: 11 Best Validation Loss: 0.6939\n",
      "Steps: 3, Timesteps: 12 Best Validation Loss: 0.6933\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular timestep\n",
    "t_loss = []\n",
    "steps = 1\n",
    "\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for t in time:\n",
    "    \n",
    "    #Generate inputs on timesteps\n",
    "    X_train_2, y_train_2 = get_inputs(X_train, y_train, batch_size, t)\n",
    "    X_val_2, y_val_2 = get_inputs(X_val, y_val, batch_size, t)\n",
    "    \n",
    "    #Build network\n",
    "    t_network = build_network(best_n_hidden, best_drop_out, input_shape=(t, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    t_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_timesteps = t_network.fit(X_train_2, y_train_2, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_val_2, y_val_2), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Timesteps: %d' % t, 'Best Validation Loss: %.4f' % np.min(fitted_timesteps.history['val_loss']))\n",
    "    \n",
    "    #Save\n",
    "    t_loss.append([np.min(fitted_timesteps.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Drop out probability: (0.10, 0.10),  Validation Loss: 0.6870\n",
      "Stored 'best_timestep' (int32)\n",
      "Stored 'best_t_loss' (float64)\n"
     ]
    }
   ],
   "source": [
    "best_timestep = time[np.argmin(t_loss)]\n",
    "best_t_loss = np.min(t_loss)\n",
    "print('Best Drop out probability: (%.2f, %.2f), ' % best_drop_out, 'Validation Loss: %.4f' % best_t_loss)\n",
    "\n",
    "%store best_timestep\n",
    "%store best_t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "-------------\n",
      "Learning Rate: 0.00001\n",
      "Number of layers:  (5, 5, 5)\n",
      "Dropout Probability: (0.10, 0.10)\n",
      "Timesteps:  10\n"
     ]
    }
   ],
   "source": [
    "print('Summary')\n",
    "print('-------------')\n",
    "print('Learning Rate: %.5f' % best_lr)\n",
    "print('Number of layers: ', best_n_hidden)\n",
    "print('Dropout Probability: (%.2f, %.2f)' % best_drop_out)\n",
    "print('Timesteps: ', best_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
